{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f97cab3",
   "metadata": {
    "papermill": {
     "duration": 0.007752,
     "end_time": "2024-09-18T17:33:47.101537",
     "exception": false,
     "start_time": "2024-09-18T17:33:47.093785",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Titanic Competition - Deep Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0abd26e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:33:47.117246Z",
     "iopub.status.busy": "2024-09-18T17:33:47.116905Z",
     "iopub.status.idle": "2024-09-18T17:33:47.128300Z",
     "shell.execute_reply": "2024-09-18T17:33:47.127392Z"
    },
    "papermill": {
     "duration": 0.021659,
     "end_time": "2024-09-18T17:33:47.130339",
     "exception": false,
     "start_time": "2024-09-18T17:33:47.108680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 100\n",
    "LR = 0.01\n",
    "BATCH_SIZE = 64\n",
    "DROPOUT = 0.3\n",
    "L2 = 0.01\n",
    "MOMENTUM = 0.9\n",
    "DECAY = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a14cd2cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:33:47.146509Z",
     "iopub.status.busy": "2024-09-18T17:33:47.145858Z",
     "iopub.status.idle": "2024-09-18T17:33:49.145814Z",
     "shell.execute_reply": "2024-09-18T17:33:49.144928Z"
    },
    "papermill": {
     "duration": 2.010269,
     "end_time": "2024-09-18T17:33:49.148069",
     "exception": false,
     "start_time": "2024-09-18T17:33:47.137800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "# Set Matplotlib defaults\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=18,\n",
    "    titlepad=10,\n",
    ")\n",
    "plt.rc(\"animation\", html=\"html5\")\n",
    "\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "import os\n",
    "\n",
    "from utils import (\n",
    "    preprocess_data,\n",
    "    MyModel,\n",
    "    model_init,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d421e770",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9c213bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:33:49.191385Z",
     "iopub.status.busy": "2024-09-18T17:33:49.190685Z",
     "iopub.status.idle": "2024-09-18T17:33:49.237352Z",
     "shell.execute_reply": "2024-09-18T17:33:49.236421Z"
    },
    "papermill": {
     "duration": 0.056909,
     "end_time": "2024-09-18T17:33:49.239675",
     "exception": false,
     "start_time": "2024-09-18T17:33:49.182766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the data\n",
    "train_data = pd.read_csv(\"../input/train.csv\").set_index(\"PassengerId\")\n",
    "test_data = pd.read_csv(\"../input/test.csv\").set_index(\"PassengerId\")\n",
    "\n",
    "X, y, X_test = preprocess_data(\n",
    "    train_data,\n",
    "    test_data,\n",
    "    label_value=\"Survived\",\n",
    "    cols_to_drop=[\"Name\", \"Ticket\", \"Cabin\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a538c9",
   "metadata": {
    "papermill": {
     "duration": 0.007425,
     "end_time": "2024-09-18T17:33:49.528696",
     "exception": false,
     "start_time": "2024-09-18T17:33:49.521271",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb910234",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:33:49.545457Z",
     "iopub.status.busy": "2024-09-18T17:33:49.545126Z",
     "iopub.status.idle": "2024-09-18T17:33:49.549233Z",
     "shell.execute_reply": "2024-09-18T17:33:49.548399Z"
    },
    "papermill": {
     "duration": 0.01474,
     "end_time": "2024-09-18T17:33:49.551028",
     "exception": false,
     "start_time": "2024-09-18T17:33:49.536288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_shape = [X.shape[1]]\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f692a7ca",
   "metadata": {
    "papermill": {
     "duration": 0.00769,
     "end_time": "2024-09-18T17:34:02.181440",
     "exception": false,
     "start_time": "2024-09-18T17:34:02.173750",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Start testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b54fc9",
   "metadata": {},
   "source": [
    "Parameters to Tune:\n",
    "\n",
    "- L2 regularization strength: 0.001, 0.005, 0.01, 0.05\n",
    "- Dropout rate: 0.2, 0.3, 0.4, 0.5\n",
    "- Learning rate: 0.01, 0.001, 0.0001\n",
    "- Momentum: 0.8, 0.9\n",
    "- Weight decay: 1e-4, 1e-5\n",
    "  - Learning rate scheduler: Experiment with different decay schedules or use adaptive learning rates (e.g., ReduceLROnPlateau)\n",
    "- Batch size: 16, 32, 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b87f84ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the K-Fold cross-validator (K=5 in this example)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# DataFrame that saves the parameters with accuracies\n",
    "accuracies_df = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"learning_rate\",\n",
    "        \"batch_size\",\n",
    "        \"l2_regularization\",\n",
    "        \"dropout_rate\",\n",
    "        \"momentum\",\n",
    "        \"weight_decay\",\n",
    "        \"accuracy\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a626bc9",
   "metadata": {},
   "source": [
    "## Key parameter tuning\n",
    "\n",
    "learning_rate, batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "507d244f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------- 1/10 -----------------------\n",
      "learning_rate: 0.1\t batch_size: 32\t accuracy: 0.8268827676773072\n",
      "----------------------- 2/10 -----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kkleist\\AppData\\Local\\Temp\\ipykernel_16864\\3281139620.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  accuracies_df = pd.concat([accuracies_df, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 0.1\t batch_size: 64\t accuracy: 0.8324901223182678\n",
      "----------------------- 3/10 -----------------------\n",
      "learning_rate: 0.05\t batch_size: 32\t accuracy: 0.8277254581451416\n",
      "----------------------- 4/10 -----------------------\n",
      "learning_rate: 0.05\t batch_size: 64\t accuracy: 0.8383834719657898\n",
      "----------------------- 5/10 -----------------------\n",
      "learning_rate: 0.01\t batch_size: 32\t accuracy: 0.8417491436004638\n",
      "----------------------- 6/10 -----------------------\n",
      "learning_rate: 0.01\t batch_size: 64\t accuracy: 0.828836464881897\n",
      "----------------------- 7/10 -----------------------\n",
      "learning_rate: 0.005\t batch_size: 32\t accuracy: 0.8316497802734375\n",
      "----------------------- 8/10 -----------------------\n",
      "learning_rate: 0.005\t batch_size: 64\t accuracy: 0.8319294929504395\n",
      "----------------------- 9/10 -----------------------\n",
      "learning_rate: 0.001\t batch_size: 32\t accuracy: 0.8288451194763183\n",
      "----------------------- 10/10 -----------------------\n",
      "learning_rate: 0.001\t batch_size: 64\t accuracy: 0.8291216969490052\n"
     ]
    }
   ],
   "source": [
    "lr_values = [0.1, 0.05, 0.01, 0.005, 0.001]\n",
    "batch_values = [32, 64]\n",
    "\n",
    "num_iter = len(lr_values) * len(batch_values)\n",
    "\n",
    "i = 1\n",
    "\n",
    "for lr in lr_values:\n",
    "    for batch in batch_values:\n",
    "        acc_list = []\n",
    "        print(f\"----------------------- {i}/{num_iter} -----------------------\")\n",
    "        for train_index, val_index in kf.split(X):\n",
    "            # Split the data into training and testing sets\n",
    "            X_train, X_val = X[train_index], X[val_index]\n",
    "            y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "            # Init the model in every iteration\n",
    "            model, early_stopping, lrs = model_init(\n",
    "                MyModel(l2=L2, dropout=DROPOUT), lr, MOMENTUM, DECAY\n",
    "            )\n",
    "\n",
    "            history = model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                batch_size=batch,\n",
    "                epochs=EPOCHS,\n",
    "                callbacks=[early_stopping, lrs],\n",
    "                verbose=0,\n",
    "            )\n",
    "\n",
    "            acc_list.append(history.history[\"binary_accuracy\"][-1])\n",
    "\n",
    "        print(f\"learning_rate: {lr}\\t batch_size: {batch}\\t accuracy: {mean(acc_list)}\")\n",
    "\n",
    "        # Create a DataFrame for the new row\n",
    "        new_row = pd.DataFrame(\n",
    "            [[lr, batch, mean(acc_list)]],\n",
    "            columns=[\n",
    "                \"learning_rate\",\n",
    "                \"batch_size\",\n",
    "                \"accuracy\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Append the row using pd.concat()\n",
    "        accuracies_df = pd.concat([accuracies_df, new_row], ignore_index=True)\n",
    "\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0641eb81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>l2_regularization</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>momentum</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.841749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.050</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.838383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.100</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.832490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.005</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.831929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.005</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.831650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.829122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.828845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.010</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.828836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.050</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.827725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.100</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.826883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   learning_rate batch_size l2_regularization dropout_rate momentum  \\\n",
       "4          0.010         32               NaN          NaN      NaN   \n",
       "3          0.050         64               NaN          NaN      NaN   \n",
       "1          0.100         64               NaN          NaN      NaN   \n",
       "7          0.005         64               NaN          NaN      NaN   \n",
       "6          0.005         32               NaN          NaN      NaN   \n",
       "9          0.001         64               NaN          NaN      NaN   \n",
       "8          0.001         32               NaN          NaN      NaN   \n",
       "5          0.010         64               NaN          NaN      NaN   \n",
       "2          0.050         32               NaN          NaN      NaN   \n",
       "0          0.100         32               NaN          NaN      NaN   \n",
       "\n",
       "  weight_decay  accuracy  \n",
       "4          NaN  0.841749  \n",
       "3          NaN  0.838383  \n",
       "1          NaN  0.832490  \n",
       "7          NaN  0.831929  \n",
       "6          NaN  0.831650  \n",
       "9          NaN  0.829122  \n",
       "8          NaN  0.828845  \n",
       "5          NaN  0.828836  \n",
       "2          NaN  0.827725  \n",
       "0          NaN  0.826883  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies_df.sort_values(by=\"accuracy\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59049388",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_df.to_csv(\"output/accuracies_tuning.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbcf575",
   "metadata": {},
   "source": [
    "Extract the parameters giving the highest value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de09fc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_df_max_row = accuracies_df.loc[\n",
    "    accuracies_df[\"accuracy\"] == max(accuracies_df[\"accuracy\"])\n",
    "]\n",
    "best_lr = accuracies_df_max_row[\"learning_rate\"].iloc[0]\n",
    "best_batch = accuracies_df_max_row[\"batch_size\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ea48d9",
   "metadata": {},
   "source": [
    "## Fine-tuning\n",
    "\n",
    "### First stage\n",
    "\n",
    "l2_regularization, dropout_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68897973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame that saves the parameters with accuracies for the first fine-tuning\n",
    "accuracies_ft1_df = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"learning_rate\",\n",
    "        \"batch_size\",\n",
    "        \"l2_regularization\",\n",
    "        \"dropout_rate\",\n",
    "        \"momentum\",\n",
    "        \"weight_decay\",\n",
    "        \"accuracy\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12fd43e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------- 1/12 -----------------------\n",
      "l2_regularization: 0.005\t dropout_rate: 0.1\t accuracy: 0.855497419834137\n",
      "----------------------- 2/12 -----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kkleist\\AppData\\Local\\Temp\\ipykernel_16864\\4205188953.py:51: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  accuracies_ft1_df = pd.concat([accuracies_ft1_df, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l2_regularization: 0.005\t dropout_rate: 0.15\t accuracy: 0.8392257690429688\n",
      "----------------------- 3/12 -----------------------\n",
      "l2_regularization: 0.005\t dropout_rate: 0.2\t accuracy: 0.8448346972465515\n",
      "----------------------- 4/12 -----------------------\n",
      "l2_regularization: 0.005\t dropout_rate: 0.25\t accuracy: 0.8456789493560791\n",
      "----------------------- 5/12 -----------------------\n",
      "l2_regularization: 0.008\t dropout_rate: 0.1\t accuracy: 0.845396089553833\n",
      "----------------------- 6/12 -----------------------\n",
      "l2_regularization: 0.008\t dropout_rate: 0.15\t accuracy: 0.8425934076309204\n",
      "----------------------- 7/12 -----------------------\n",
      "l2_regularization: 0.008\t dropout_rate: 0.2\t accuracy: 0.8439955353736878\n",
      "----------------------- 8/12 -----------------------\n",
      "l2_regularization: 0.008\t dropout_rate: 0.25\t accuracy: 0.8428683996200561\n",
      "----------------------- 9/12 -----------------------\n",
      "l2_regularization: 0.01\t dropout_rate: 0.1\t accuracy: 0.841187345981598\n",
      "----------------------- 10/12 -----------------------\n",
      "l2_regularization: 0.01\t dropout_rate: 0.15\t accuracy: 0.8470814943313598\n",
      "----------------------- 11/12 -----------------------\n",
      "l2_regularization: 0.01\t dropout_rate: 0.2\t accuracy: 0.839784812927246\n",
      "----------------------- 12/12 -----------------------\n",
      "l2_regularization: 0.01\t dropout_rate: 0.25\t accuracy: 0.8445549726486206\n"
     ]
    }
   ],
   "source": [
    "l2_values = [0.005, 0.008, 0.01]\n",
    "dropout_values = [0.1, 0.15, 0.2, 0.25]\n",
    "\n",
    "num_iter = len(l2_values) * len(dropout_values)\n",
    "\n",
    "i = 1\n",
    "\n",
    "for l2 in l2_values:\n",
    "    for dropout in dropout_values:\n",
    "        acc_list = []\n",
    "        print(f\"----------------------- {i}/{num_iter} -----------------------\")\n",
    "        for train_index, val_index in kf.split(X):\n",
    "            # Split the data into training and testing sets\n",
    "            X_train, X_val = X[train_index], X[val_index]\n",
    "            y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "            # Init the model in every iteration\n",
    "            model, early_stopping, lrs = model_init(\n",
    "                MyModel(l2=l2, dropout=dropout), best_lr, MOMENTUM, DECAY\n",
    "            )\n",
    "\n",
    "            history = model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                batch_size=best_batch,\n",
    "                epochs=EPOCHS,\n",
    "                callbacks=[early_stopping, lrs],\n",
    "                verbose=0,\n",
    "            )\n",
    "\n",
    "            acc_list.append(history.history[\"binary_accuracy\"][-1])\n",
    "\n",
    "        print(\n",
    "            f\"l2_regularization: {l2}\\t dropout_rate: {dropout}\\t accuracy: {mean(acc_list)}\"\n",
    "        )\n",
    "\n",
    "        # Create a DataFrame for the new row\n",
    "        new_row = pd.DataFrame(\n",
    "            [[best_lr, best_batch, l2, dropout, mean(acc_list)]],\n",
    "            columns=[\n",
    "                \"learning_rate\",\n",
    "                \"batch_size\",\n",
    "                \"l2_regularization\",\n",
    "                \"dropout_rate\",\n",
    "                \"accuracy\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Append the row using pd.concat()\n",
    "        accuracies_ft1_df = pd.concat([accuracies_ft1_df, new_row], ignore_index=True)\n",
    "\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b173f8b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>l2_regularization</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>momentum</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.855497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.847081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.845679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.845396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.844835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.844555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.843996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.842868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.842593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.841187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.839785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.839226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    learning_rate batch_size  l2_regularization  dropout_rate momentum  \\\n",
       "0            0.01         32              0.005          0.10      NaN   \n",
       "9            0.01         32              0.010          0.15      NaN   \n",
       "3            0.01         32              0.005          0.25      NaN   \n",
       "4            0.01         32              0.008          0.10      NaN   \n",
       "2            0.01         32              0.005          0.20      NaN   \n",
       "11           0.01         32              0.010          0.25      NaN   \n",
       "6            0.01         32              0.008          0.20      NaN   \n",
       "7            0.01         32              0.008          0.25      NaN   \n",
       "5            0.01         32              0.008          0.15      NaN   \n",
       "8            0.01         32              0.010          0.10      NaN   \n",
       "10           0.01         32              0.010          0.20      NaN   \n",
       "1            0.01         32              0.005          0.15      NaN   \n",
       "\n",
       "   weight_decay  accuracy  \n",
       "0           NaN  0.855497  \n",
       "9           NaN  0.847081  \n",
       "3           NaN  0.845679  \n",
       "4           NaN  0.845396  \n",
       "2           NaN  0.844835  \n",
       "11          NaN  0.844555  \n",
       "6           NaN  0.843996  \n",
       "7           NaN  0.842868  \n",
       "5           NaN  0.842593  \n",
       "8           NaN  0.841187  \n",
       "10          NaN  0.839785  \n",
       "1           NaN  0.839226  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies_ft1_df.sort_values(by=\"accuracy\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f54cea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_ft1_df.to_csv(\"output/accuracies_fine_tuning_1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9a1952",
   "metadata": {},
   "source": [
    "Extract the parameters giving the highest value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44b0bd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_df_max_row = accuracies_ft1_df.loc[\n",
    "    accuracies_ft1_df[\"accuracy\"] == max(accuracies_ft1_df[\"accuracy\"])\n",
    "]\n",
    "best_l2 = accuracies_df_max_row[\"l2_regularization\"].iloc[0]\n",
    "best_dropout = accuracies_df_max_row[\"dropout_rate\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6df00c",
   "metadata": {},
   "source": [
    "## Fine-tuning\n",
    "\n",
    "### Second stage\n",
    "\n",
    "momentum, weight_decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d7c3f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame that saves the parameters with accuracies for the first fine-tuning\n",
    "accuracies_ft2_df = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"learning_rate\",\n",
    "        \"batch_size\",\n",
    "        \"l2_regularization\",\n",
    "        \"dropout_rate\",\n",
    "        \"momentum\",\n",
    "        \"weight_decay\",\n",
    "        \"accuracy\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af32ebba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------- 1/12 -----------------------\n",
      "momentum: 0.85\t weight_decay: 1e-05\t accuracy: 0.8504436135292053\n",
      "----------------------- 2/12 -----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kkleist\\AppData\\Local\\Temp\\ipykernel_16864\\1758824499.py:63: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  accuracies_ft2_df = pd.concat([accuracies_ft2_df, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "momentum: 0.85\t weight_decay: 5e-05\t accuracy: 0.845396876335144\n",
      "----------------------- 3/12 -----------------------\n",
      "momentum: 0.85\t weight_decay: 0.0001\t accuracy: 0.8484800696372986\n",
      "----------------------- 4/12 -----------------------\n",
      "momentum: 0.85\t weight_decay: 0.0005\t accuracy: 0.8501619219779968\n",
      "----------------------- 5/12 -----------------------\n",
      "momentum: 0.88\t weight_decay: 1e-05\t accuracy: 0.8597034096717835\n",
      "----------------------- 6/12 -----------------------\n",
      "momentum: 0.88\t weight_decay: 5e-05\t accuracy: 0.8515668153762818\n",
      "----------------------- 7/12 -----------------------\n",
      "momentum: 0.88\t weight_decay: 0.0001\t accuracy: 0.8563365817070008\n",
      "----------------------- 8/12 -----------------------\n",
      "momentum: 0.88\t weight_decay: 0.0005\t accuracy: 0.8532553553581238\n",
      "----------------------- 9/12 -----------------------\n",
      "momentum: 0.9\t weight_decay: 1e-05\t accuracy: 0.8476413249969482\n",
      "----------------------- 10/12 -----------------------\n",
      "momentum: 0.9\t weight_decay: 5e-05\t accuracy: 0.8580235242843628\n",
      "----------------------- 11/12 -----------------------\n",
      "momentum: 0.9\t weight_decay: 0.0001\t accuracy: 0.8524091124534607\n",
      "----------------------- 12/12 -----------------------\n",
      "momentum: 0.9\t weight_decay: 0.0005\t accuracy: 0.8532518029212952\n"
     ]
    }
   ],
   "source": [
    "momentum_values = [0.85, 0.88, 0.9]\n",
    "decay_values = [1e-5, 5e-5, 1e-4, 5e-4]\n",
    "\n",
    "num_iter = len(momentum_values) * len(decay_values)\n",
    "\n",
    "i = 1\n",
    "\n",
    "for momentum in momentum_values:\n",
    "    for decay in decay_values:\n",
    "        acc_list = []\n",
    "        print(f\"----------------------- {i}/{num_iter} -----------------------\")\n",
    "        for train_index, val_index in kf.split(X):\n",
    "            # Split the data into training and testing sets\n",
    "            X_train, X_val = X[train_index], X[val_index]\n",
    "            y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "            # Init the model in every iteration\n",
    "            model, early_stopping, lrs = model_init(\n",
    "                MyModel(l2=best_l2, dropout=best_dropout), best_lr, momentum, decay\n",
    "            )\n",
    "\n",
    "            history = model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                batch_size=best_batch,\n",
    "                epochs=EPOCHS,\n",
    "                callbacks=[early_stopping, lrs],\n",
    "                verbose=0,\n",
    "            )\n",
    "\n",
    "            acc_list.append(history.history[\"binary_accuracy\"][-1])\n",
    "\n",
    "        print(\n",
    "            f\"momentum: {momentum}\\t weight_decay: {decay}\\t accuracy: {mean(acc_list)}\"\n",
    "        )\n",
    "\n",
    "        # Create a DataFrame for the new row\n",
    "        new_row = pd.DataFrame(\n",
    "            [\n",
    "                [\n",
    "                    best_lr,\n",
    "                    best_batch,\n",
    "                    best_l2,\n",
    "                    best_dropout,\n",
    "                    momentum,\n",
    "                    decay,\n",
    "                    mean(acc_list),\n",
    "                ]\n",
    "            ],\n",
    "            columns=[\n",
    "                \"learning_rate\",\n",
    "                \"batch_size\",\n",
    "                \"l2_regularization\",\n",
    "                \"dropout_rate\",\n",
    "                \"momentum\",\n",
    "                \"weight_decay\",\n",
    "                \"accuracy\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Append the row using pd.concat()\n",
    "        accuracies_ft2_df = pd.concat([accuracies_ft2_df, new_row], ignore_index=True)\n",
    "        \n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "823fed6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>l2_regularization</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>momentum</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.859703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.858024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.856337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>0.853255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>0.853252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.852409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.851567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.850444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>0.850162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.848480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.847641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.845397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    learning_rate batch_size  l2_regularization  dropout_rate  momentum  \\\n",
       "4            0.01         32              0.005           0.1      0.88   \n",
       "9            0.01         32              0.005           0.1      0.90   \n",
       "6            0.01         32              0.005           0.1      0.88   \n",
       "7            0.01         32              0.005           0.1      0.88   \n",
       "11           0.01         32              0.005           0.1      0.90   \n",
       "10           0.01         32              0.005           0.1      0.90   \n",
       "5            0.01         32              0.005           0.1      0.88   \n",
       "0            0.01         32              0.005           0.1      0.85   \n",
       "3            0.01         32              0.005           0.1      0.85   \n",
       "2            0.01         32              0.005           0.1      0.85   \n",
       "8            0.01         32              0.005           0.1      0.90   \n",
       "1            0.01         32              0.005           0.1      0.85   \n",
       "\n",
       "    weight_decay  accuracy  \n",
       "4        0.00001  0.859703  \n",
       "9        0.00005  0.858024  \n",
       "6        0.00010  0.856337  \n",
       "7        0.00050  0.853255  \n",
       "11       0.00050  0.853252  \n",
       "10       0.00010  0.852409  \n",
       "5        0.00005  0.851567  \n",
       "0        0.00001  0.850444  \n",
       "3        0.00050  0.850162  \n",
       "2        0.00010  0.848480  \n",
       "8        0.00001  0.847641  \n",
       "1        0.00005  0.845397  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies_ft2_df.sort_values(by=\"accuracy\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cfad0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_ft2_df.to_csv(\"output/accuracies_fine_tuning_2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6598f8b4",
   "metadata": {},
   "source": [
    "Extract the parameters giving the highest value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9066974d",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_df_max_row = accuracies_ft2_df.loc[\n",
    "    accuracies_ft2_df[\"accuracy\"] == max(accuracies_ft2_df[\"accuracy\"])\n",
    "]\n",
    "best_momentum = accuracies_df_max_row[\"momentum\"].iloc[0]\n",
    "best_decay = accuracies_df_max_row[\"weight_decay\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1f5aa9",
   "metadata": {
    "papermill": {
     "duration": 0.10352,
     "end_time": "2024-09-18T17:34:55.747936",
     "exception": false,
     "start_time": "2024-09-18T17:34:55.644416",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submit prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05d5aaca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:34:55.957156Z",
     "iopub.status.busy": "2024-09-18T17:34:55.956757Z",
     "iopub.status.idle": "2024-09-18T17:34:58.806856Z",
     "shell.execute_reply": "2024-09-18T17:34:58.805781Z"
    },
    "papermill": {
     "duration": 2.957386,
     "end_time": "2024-09-18T17:34:58.809153",
     "exception": false,
     "start_time": "2024-09-18T17:34:55.851767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - binary_accuracy: 0.6770 - loss: 2.0556 - learning_rate: 0.0100\n",
      "Epoch 2/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8159 - loss: 1.7999 - learning_rate: 0.0100\n",
      "Epoch 3/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.7996 - loss: 1.7143 - learning_rate: 0.0100\n",
      "Epoch 4/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8319 - loss: 1.6100 - learning_rate: 0.0100\n",
      "Epoch 5/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8249 - loss: 1.5802 - learning_rate: 0.0100\n",
      "Epoch 6/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.7994 - loss: 1.5680 - learning_rate: 0.0100\n",
      "Epoch 7/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8146 - loss: 1.4637 - learning_rate: 0.0100\n",
      "Epoch 8/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8399 - loss: 1.4238 - learning_rate: 0.0100\n",
      "Epoch 9/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8327 - loss: 1.3856 - learning_rate: 0.0100\n",
      "Epoch 10/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8228 - loss: 1.3348 - learning_rate: 0.0100\n",
      "Epoch 11/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8143 - loss: 1.3060 - learning_rate: 0.0100\n",
      "Epoch 12/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8131 - loss: 1.2785 - learning_rate: 0.0100\n",
      "Epoch 13/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8073 - loss: 1.2506 - learning_rate: 0.0100\n",
      "Epoch 14/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8300 - loss: 1.1859 - learning_rate: 0.0100\n",
      "Epoch 15/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8126 - loss: 1.1353 - learning_rate: 0.0100\n",
      "Epoch 16/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8335 - loss: 1.1028 - learning_rate: 0.0100\n",
      "Epoch 17/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8165 - loss: 1.0912 - learning_rate: 0.0100\n",
      "Epoch 18/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8239 - loss: 1.0656 - learning_rate: 0.0100\n",
      "Epoch 19/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8057 - loss: 1.0650 - learning_rate: 0.0100\n",
      "Epoch 20/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8312 - loss: 0.9928 - learning_rate: 0.0100\n",
      "Epoch 21/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8324 - loss: 0.9656 - learning_rate: 0.0100\n",
      "Epoch 22/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8184 - loss: 0.9727 - learning_rate: 0.0100\n",
      "Epoch 23/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8037 - loss: 0.9818 - learning_rate: 0.0100\n",
      "Epoch 24/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8237 - loss: 0.8997 - learning_rate: 0.0100\n",
      "Epoch 25/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8251 - loss: 0.8906 - learning_rate: 0.0100\n",
      "Epoch 26/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8047 - loss: 0.8991 - learning_rate: 0.0100\n",
      "Epoch 27/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8326 - loss: 0.8335 - learning_rate: 0.0100\n",
      "Epoch 28/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8333 - loss: 0.8284 - learning_rate: 0.0100\n",
      "Epoch 29/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8307 - loss: 0.7903 - learning_rate: 0.0100\n",
      "Epoch 30/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8251 - loss: 0.8014 - learning_rate: 0.0100\n",
      "Epoch 31/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8286 - loss: 0.7616 - learning_rate: 0.0100\n",
      "Epoch 32/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8379 - loss: 0.7408 - learning_rate: 0.0100\n",
      "Epoch 33/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8098 - loss: 0.7561 - learning_rate: 0.0100\n",
      "Epoch 34/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8490 - loss: 0.6906 - learning_rate: 0.0100\n",
      "Epoch 35/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8216 - loss: 0.6993 - learning_rate: 0.0100\n",
      "Epoch 36/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8217 - loss: 0.7006 - learning_rate: 0.0100\n",
      "Epoch 37/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8453 - loss: 0.6730 - learning_rate: 0.0100\n",
      "Epoch 38/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8122 - loss: 0.6827 - learning_rate: 0.0100\n",
      "Epoch 39/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8379 - loss: 0.6579 - learning_rate: 0.0100\n",
      "Epoch 40/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8335 - loss: 0.6602 - learning_rate: 0.0100\n",
      "Epoch 41/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8287 - loss: 0.6479 - learning_rate: 0.0100\n",
      "Epoch 42/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8487 - loss: 0.5962 - learning_rate: 0.0100\n",
      "Epoch 43/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8249 - loss: 0.6600 - learning_rate: 0.0100\n",
      "Epoch 44/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8379 - loss: 0.6047 - learning_rate: 0.0100\n",
      "Epoch 45/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8095 - loss: 0.6113 - learning_rate: 0.0100\n",
      "Epoch 46/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8026 - loss: 0.6375 - learning_rate: 0.0100\n",
      "Epoch 47/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8422 - loss: 0.5635 - learning_rate: 0.0100\n",
      "Epoch 48/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8400 - loss: 0.5772 - learning_rate: 0.0100\n",
      "Epoch 49/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8114 - loss: 0.5925 - learning_rate: 0.0100\n",
      "Epoch 50/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8247 - loss: 0.5658 - learning_rate: 0.0100\n",
      "Epoch 51/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8165 - loss: 0.5888 - learning_rate: 0.0100\n",
      "Epoch 52/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8336 - loss: 0.5747 - learning_rate: 0.0100\n",
      "Epoch 53/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8168 - loss: 0.5784 - learning_rate: 0.0100\n",
      "Epoch 54/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8239 - loss: 0.5552 - learning_rate: 0.0100\n",
      "Epoch 55/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8374 - loss: 0.5367 - learning_rate: 0.0100\n",
      "Epoch 56/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8342 - loss: 0.5294 - learning_rate: 0.0100\n",
      "Epoch 57/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8119 - loss: 0.5864 - learning_rate: 0.0100\n",
      "Epoch 58/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8222 - loss: 0.5494 - learning_rate: 0.0100\n",
      "Epoch 59/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8327 - loss: 0.5185 - learning_rate: 0.0100\n",
      "Epoch 60/60\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8375 - loss: 0.5172 - learning_rate: 0.0100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Your submission was successfully saved!\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "model, early_stopping, lrs = model_init(\n",
    "    MyModel(l2=best_l2, dropout=best_dropout), best_lr, best_momentum, best_decay\n",
    ")\n",
    "model.fit(\n",
    "    X,\n",
    "    y,\n",
    "    batch_size=best_batch,\n",
    "    epochs=52,\n",
    "    callbacks=[early_stopping, lrs],\n",
    ")\n",
    "\n",
    "# Get predictions\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(np.intc)\n",
    "predictions = np.concatenate(y_pred)\n",
    "\n",
    "# Saving the predictions\n",
    "output = pd.DataFrame({\"PassengerId\": test_data.index, \"Survived\": predictions})\n",
    "if not os.path.isdir(\"output/\"):\n",
    "    os.mkdir(\"output/\")\n",
    "output.to_csv(\"output/submission.csv\", index=False)\n",
    "\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc46bbad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 26502,
     "sourceId": 3136,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "titanic-comp-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 77.245806,
   "end_time": "2024-09-18T17:35:01.531921",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-18T17:33:44.286115",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
