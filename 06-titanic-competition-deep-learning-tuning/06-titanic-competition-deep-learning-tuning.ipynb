{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f97cab3",
   "metadata": {
    "papermill": {
     "duration": 0.007752,
     "end_time": "2024-09-18T17:33:47.101537",
     "exception": false,
     "start_time": "2024-09-18T17:33:47.093785",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Titanic Competition - Deep Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0abd26e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:33:47.117246Z",
     "iopub.status.busy": "2024-09-18T17:33:47.116905Z",
     "iopub.status.idle": "2024-09-18T17:33:47.128300Z",
     "shell.execute_reply": "2024-09-18T17:33:47.127392Z"
    },
    "papermill": {
     "duration": 0.021659,
     "end_time": "2024-09-18T17:33:47.130339",
     "exception": false,
     "start_time": "2024-09-18T17:33:47.108680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 100\n",
    "LR = 0.01\n",
    "BATCH_SIZE = 64\n",
    "DROPOUT = 0.3\n",
    "L2 = 0.01\n",
    "MOMENTUM = 0.9\n",
    "DECAY = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a14cd2cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:33:47.146509Z",
     "iopub.status.busy": "2024-09-18T17:33:47.145858Z",
     "iopub.status.idle": "2024-09-18T17:33:49.145814Z",
     "shell.execute_reply": "2024-09-18T17:33:49.144928Z"
    },
    "papermill": {
     "duration": 2.010269,
     "end_time": "2024-09-18T17:33:49.148069",
     "exception": false,
     "start_time": "2024-09-18T17:33:47.137800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "# Set Matplotlib defaults\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=18,\n",
    "    titlepad=10,\n",
    ")\n",
    "plt.rc(\"animation\", html=\"html5\")\n",
    "\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "import os\n",
    "\n",
    "from utils import (\n",
    "    preprocess_data,\n",
    "    MyModel,\n",
    "    model_init,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d421e770",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9c213bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:33:49.191385Z",
     "iopub.status.busy": "2024-09-18T17:33:49.190685Z",
     "iopub.status.idle": "2024-09-18T17:33:49.237352Z",
     "shell.execute_reply": "2024-09-18T17:33:49.236421Z"
    },
    "papermill": {
     "duration": 0.056909,
     "end_time": "2024-09-18T17:33:49.239675",
     "exception": false,
     "start_time": "2024-09-18T17:33:49.182766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the data\n",
    "train_data = pd.read_csv(\"../input/train.csv\").set_index(\"PassengerId\")\n",
    "test_data = pd.read_csv(\"../input/test.csv\").set_index(\"PassengerId\")\n",
    "\n",
    "X, y, X_test = preprocess_data(\n",
    "    train_data,\n",
    "    test_data,\n",
    "    label_value=\"Survived\",\n",
    "    cols_to_drop=[\"Name\", \"Ticket\", \"Cabin\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a538c9",
   "metadata": {
    "papermill": {
     "duration": 0.007425,
     "end_time": "2024-09-18T17:33:49.528696",
     "exception": false,
     "start_time": "2024-09-18T17:33:49.521271",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb910234",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:33:49.545457Z",
     "iopub.status.busy": "2024-09-18T17:33:49.545126Z",
     "iopub.status.idle": "2024-09-18T17:33:49.549233Z",
     "shell.execute_reply": "2024-09-18T17:33:49.548399Z"
    },
    "papermill": {
     "duration": 0.01474,
     "end_time": "2024-09-18T17:33:49.551028",
     "exception": false,
     "start_time": "2024-09-18T17:33:49.536288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_shape = [X.shape[1]]\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f692a7ca",
   "metadata": {
    "papermill": {
     "duration": 0.00769,
     "end_time": "2024-09-18T17:34:02.181440",
     "exception": false,
     "start_time": "2024-09-18T17:34:02.173750",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Start testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b54fc9",
   "metadata": {},
   "source": [
    "Parameters to Tune:\n",
    "\n",
    "- L2 regularization strength: 0.001, 0.005, 0.01, 0.05\n",
    "- Dropout rate: 0.2, 0.3, 0.4, 0.5\n",
    "- Learning rate: 0.01, 0.001, 0.0001\n",
    "- Momentum: 0.8, 0.9\n",
    "- Weight decay: 1e-4, 1e-5\n",
    "  - Learning rate scheduler: Experiment with different decay schedules or use adaptive learning rates (e.g., ReduceLROnPlateau)\n",
    "- Batch size: 16, 32, 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b87f84ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the K-Fold cross-validator (K=5 in this example)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# DataFrame that saves the parameters with accuracies\n",
    "accuracies_df = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"learning_rate\",\n",
    "        \"batch_size\",\n",
    "        \"l2_regularization\",\n",
    "        \"dropout_rate\",\n",
    "        \"momentum\",\n",
    "        \"weight_decay\",\n",
    "        \"accuracy\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a626bc9",
   "metadata": {},
   "source": [
    "## Key parameter tuning\n",
    "\n",
    "learning_rate, batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "507d244f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------- 1/10 -----------------------\n",
      "learning_rate: 0.1\t batch_size: 32\t accuracy: 0.8268827676773072\n",
      "----------------------- 2/10 -----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kkleist\\AppData\\Local\\Temp\\ipykernel_16864\\3281139620.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  accuracies_df = pd.concat([accuracies_df, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 0.1\t batch_size: 64\t accuracy: 0.8324901223182678\n",
      "----------------------- 3/10 -----------------------\n",
      "learning_rate: 0.05\t batch_size: 32\t accuracy: 0.8277254581451416\n",
      "----------------------- 4/10 -----------------------\n",
      "learning_rate: 0.05\t batch_size: 64\t accuracy: 0.8383834719657898\n",
      "----------------------- 5/10 -----------------------\n",
      "learning_rate: 0.01\t batch_size: 32\t accuracy: 0.8417491436004638\n",
      "----------------------- 6/10 -----------------------\n",
      "learning_rate: 0.01\t batch_size: 64\t accuracy: 0.828836464881897\n",
      "----------------------- 7/10 -----------------------\n",
      "learning_rate: 0.005\t batch_size: 32\t accuracy: 0.8316497802734375\n",
      "----------------------- 8/10 -----------------------\n",
      "learning_rate: 0.005\t batch_size: 64\t accuracy: 0.8319294929504395\n",
      "----------------------- 9/10 -----------------------\n",
      "learning_rate: 0.001\t batch_size: 32\t accuracy: 0.8288451194763183\n",
      "----------------------- 10/10 -----------------------\n",
      "learning_rate: 0.001\t batch_size: 64\t accuracy: 0.8291216969490052\n"
     ]
    }
   ],
   "source": [
    "lr_values = [0.1, 0.05, 0.01, 0.005, 0.001]\n",
    "batch_values = [32, 64]\n",
    "\n",
    "num_iter = len(lr_values) * len(batch_values)\n",
    "\n",
    "i = 1\n",
    "\n",
    "for lr in lr_values:\n",
    "    for batch in batch_values:\n",
    "        acc_list = []\n",
    "        print(f\"----------------------- {i}/{num_iter} -----------------------\")\n",
    "        for train_index, val_index in kf.split(X):\n",
    "            # Split the data into training and testing sets\n",
    "            X_train, X_val = X[train_index], X[val_index]\n",
    "            y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "            # Init the model in every iteration\n",
    "            model, early_stopping, lrs = model_init(\n",
    "                MyModel(l2=L2, dropout=DROPOUT), lr, MOMENTUM, DECAY\n",
    "            )\n",
    "\n",
    "            history = model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                batch_size=batch,\n",
    "                epochs=EPOCHS,\n",
    "                callbacks=[early_stopping, lrs],\n",
    "                verbose=0,\n",
    "            )\n",
    "\n",
    "            acc_list.append(history.history[\"binary_accuracy\"][-1])\n",
    "\n",
    "        print(f\"learning_rate: {lr}\\t batch_size: {batch}\\t accuracy: {mean(acc_list)}\")\n",
    "\n",
    "        # Create a DataFrame for the new row\n",
    "        new_row = pd.DataFrame(\n",
    "            [[lr, batch, mean(acc_list)]],\n",
    "            columns=[\n",
    "                \"learning_rate\",\n",
    "                \"batch_size\",\n",
    "                \"accuracy\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Append the row using pd.concat()\n",
    "        accuracies_df = pd.concat([accuracies_df, new_row], ignore_index=True)\n",
    "\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0641eb81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>l2_regularization</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>momentum</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.841749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.050</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.838383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.100</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.832490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.005</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.831929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.005</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.831650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.829122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.828845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.010</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.828836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.050</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.827725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.100</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.826883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   learning_rate batch_size l2_regularization dropout_rate momentum  \\\n",
       "4          0.010         32               NaN          NaN      NaN   \n",
       "3          0.050         64               NaN          NaN      NaN   \n",
       "1          0.100         64               NaN          NaN      NaN   \n",
       "7          0.005         64               NaN          NaN      NaN   \n",
       "6          0.005         32               NaN          NaN      NaN   \n",
       "9          0.001         64               NaN          NaN      NaN   \n",
       "8          0.001         32               NaN          NaN      NaN   \n",
       "5          0.010         64               NaN          NaN      NaN   \n",
       "2          0.050         32               NaN          NaN      NaN   \n",
       "0          0.100         32               NaN          NaN      NaN   \n",
       "\n",
       "  weight_decay  accuracy  \n",
       "4          NaN  0.841749  \n",
       "3          NaN  0.838383  \n",
       "1          NaN  0.832490  \n",
       "7          NaN  0.831929  \n",
       "6          NaN  0.831650  \n",
       "9          NaN  0.829122  \n",
       "8          NaN  0.828845  \n",
       "5          NaN  0.828836  \n",
       "2          NaN  0.827725  \n",
       "0          NaN  0.826883  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies_df.sort_values(by=\"accuracy\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59049388",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_df.to_csv(\"output/accuracies_tuning.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbcf575",
   "metadata": {},
   "source": [
    "Extract the parameters giving the highest value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de09fc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_df_max_row = accuracies_df.loc[\n",
    "    accuracies_df[\"accuracy\"] == max(accuracies_df[\"accuracy\"])\n",
    "]\n",
    "best_lr = accuracies_df_max_row[\"learning_rate\"].iloc[0]\n",
    "best_batch = accuracies_df_max_row[\"batch_size\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ea48d9",
   "metadata": {},
   "source": [
    "## Fine-tuning\n",
    "\n",
    "### First stage\n",
    "\n",
    "l2_regularization, dropout_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68897973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame that saves the parameters with accuracies for the first fine-tuning\n",
    "accuracies_ft1_df = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"learning_rate\",\n",
    "        \"batch_size\",\n",
    "        \"l2_regularization\",\n",
    "        \"dropout_rate\",\n",
    "        \"momentum\",\n",
    "        \"weight_decay\",\n",
    "        \"accuracy\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12fd43e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------- 1/12 -----------------------\n",
      "l2_regularization: 0.005\t dropout_rate: 0.1\t accuracy: 0.855497419834137\n",
      "----------------------- 2/12 -----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kkleist\\AppData\\Local\\Temp\\ipykernel_16864\\4205188953.py:51: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  accuracies_ft1_df = pd.concat([accuracies_ft1_df, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l2_regularization: 0.005\t dropout_rate: 0.15\t accuracy: 0.8392257690429688\n",
      "----------------------- 3/12 -----------------------\n",
      "l2_regularization: 0.005\t dropout_rate: 0.2\t accuracy: 0.8448346972465515\n",
      "----------------------- 4/12 -----------------------\n",
      "l2_regularization: 0.005\t dropout_rate: 0.25\t accuracy: 0.8456789493560791\n",
      "----------------------- 5/12 -----------------------\n",
      "l2_regularization: 0.008\t dropout_rate: 0.1\t accuracy: 0.845396089553833\n",
      "----------------------- 6/12 -----------------------\n",
      "l2_regularization: 0.008\t dropout_rate: 0.15\t accuracy: 0.8425934076309204\n",
      "----------------------- 7/12 -----------------------\n",
      "l2_regularization: 0.008\t dropout_rate: 0.2\t accuracy: 0.8439955353736878\n",
      "----------------------- 8/12 -----------------------\n",
      "l2_regularization: 0.008\t dropout_rate: 0.25\t accuracy: 0.8428683996200561\n",
      "----------------------- 9/12 -----------------------\n",
      "l2_regularization: 0.01\t dropout_rate: 0.1\t accuracy: 0.841187345981598\n",
      "----------------------- 10/12 -----------------------\n",
      "l2_regularization: 0.01\t dropout_rate: 0.15\t accuracy: 0.8470814943313598\n",
      "----------------------- 11/12 -----------------------\n",
      "l2_regularization: 0.01\t dropout_rate: 0.2\t accuracy: 0.839784812927246\n",
      "----------------------- 12/12 -----------------------\n",
      "l2_regularization: 0.01\t dropout_rate: 0.25\t accuracy: 0.8445549726486206\n"
     ]
    }
   ],
   "source": [
    "l2_values = [0.005, 0.008, 0.01]\n",
    "dropout_values = [0.1, 0.15, 0.2, 0.25]\n",
    "\n",
    "num_iter = len(l2_values) * len(dropout_values)\n",
    "\n",
    "i = 1\n",
    "\n",
    "for l2 in l2_values:\n",
    "    for dropout in dropout_values:\n",
    "        acc_list = []\n",
    "        print(f\"----------------------- {i}/{num_iter} -----------------------\")\n",
    "        for train_index, val_index in kf.split(X):\n",
    "            # Split the data into training and testing sets\n",
    "            X_train, X_val = X[train_index], X[val_index]\n",
    "            y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "            # Init the model in every iteration\n",
    "            model, early_stopping, lrs = model_init(\n",
    "                MyModel(l2=l2, dropout=dropout), best_lr, MOMENTUM, DECAY\n",
    "            )\n",
    "\n",
    "            history = model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                batch_size=best_batch,\n",
    "                epochs=EPOCHS,\n",
    "                callbacks=[early_stopping, lrs],\n",
    "                verbose=0,\n",
    "            )\n",
    "\n",
    "            acc_list.append(history.history[\"binary_accuracy\"][-1])\n",
    "\n",
    "        print(\n",
    "            f\"l2_regularization: {l2}\\t dropout_rate: {dropout}\\t accuracy: {mean(acc_list)}\"\n",
    "        )\n",
    "\n",
    "        # Create a DataFrame for the new row\n",
    "        new_row = pd.DataFrame(\n",
    "            [[best_lr, best_batch, l2, dropout, mean(acc_list)]],\n",
    "            columns=[\n",
    "                \"learning_rate\",\n",
    "                \"batch_size\",\n",
    "                \"l2_regularization\",\n",
    "                \"dropout_rate\",\n",
    "                \"accuracy\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Append the row using pd.concat()\n",
    "        accuracies_ft1_df = pd.concat([accuracies_ft1_df, new_row], ignore_index=True)\n",
    "\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b173f8b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>l2_regularization</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>momentum</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.855497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.847081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.845679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.845396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.844835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.844555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.843996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.842868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.842593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.841187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.839785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.839226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    learning_rate batch_size  l2_regularization  dropout_rate momentum  \\\n",
       "0            0.01         32              0.005          0.10      NaN   \n",
       "9            0.01         32              0.010          0.15      NaN   \n",
       "3            0.01         32              0.005          0.25      NaN   \n",
       "4            0.01         32              0.008          0.10      NaN   \n",
       "2            0.01         32              0.005          0.20      NaN   \n",
       "11           0.01         32              0.010          0.25      NaN   \n",
       "6            0.01         32              0.008          0.20      NaN   \n",
       "7            0.01         32              0.008          0.25      NaN   \n",
       "5            0.01         32              0.008          0.15      NaN   \n",
       "8            0.01         32              0.010          0.10      NaN   \n",
       "10           0.01         32              0.010          0.20      NaN   \n",
       "1            0.01         32              0.005          0.15      NaN   \n",
       "\n",
       "   weight_decay  accuracy  \n",
       "0           NaN  0.855497  \n",
       "9           NaN  0.847081  \n",
       "3           NaN  0.845679  \n",
       "4           NaN  0.845396  \n",
       "2           NaN  0.844835  \n",
       "11          NaN  0.844555  \n",
       "6           NaN  0.843996  \n",
       "7           NaN  0.842868  \n",
       "5           NaN  0.842593  \n",
       "8           NaN  0.841187  \n",
       "10          NaN  0.839785  \n",
       "1           NaN  0.839226  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies_ft1_df.sort_values(by=\"accuracy\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f54cea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_ft1_df.to_csv(\"output/accuracies_fine_tuning_1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9a1952",
   "metadata": {},
   "source": [
    "Extract the parameters giving the highest value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44b0bd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_df_max_row = accuracies_ft1_df.loc[\n",
    "    accuracies_ft1_df[\"accuracy\"] == max(accuracies_ft1_df[\"accuracy\"])\n",
    "]\n",
    "best_l2 = accuracies_df_max_row[\"l2_regularization\"].iloc[0]\n",
    "best_dropout = accuracies_df_max_row[\"dropout_rate\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6df00c",
   "metadata": {},
   "source": [
    "## Fine-tuning\n",
    "\n",
    "### Second stage\n",
    "\n",
    "momentum, weight_decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d7c3f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame that saves the parameters with accuracies for the first fine-tuning\n",
    "accuracies_ft2_df = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"learning_rate\",\n",
    "        \"batch_size\",\n",
    "        \"l2_regularization\",\n",
    "        \"dropout_rate\",\n",
    "        \"momentum\",\n",
    "        \"weight_decay\",\n",
    "        \"accuracy\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af32ebba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------- 1/12 -----------------------\n",
      "momentum: 0.85\t weight_decay: 1e-05\t accuracy: 0.8504436135292053\n",
      "----------------------- 2/12 -----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kkleist\\AppData\\Local\\Temp\\ipykernel_16864\\1758824499.py:63: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  accuracies_ft2_df = pd.concat([accuracies_ft2_df, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "momentum: 0.85\t weight_decay: 5e-05\t accuracy: 0.845396876335144\n",
      "----------------------- 3/12 -----------------------\n",
      "momentum: 0.85\t weight_decay: 0.0001\t accuracy: 0.8484800696372986\n",
      "----------------------- 4/12 -----------------------\n",
      "momentum: 0.85\t weight_decay: 0.0005\t accuracy: 0.8501619219779968\n",
      "----------------------- 5/12 -----------------------\n",
      "momentum: 0.88\t weight_decay: 1e-05\t accuracy: 0.8597034096717835\n",
      "----------------------- 6/12 -----------------------\n",
      "momentum: 0.88\t weight_decay: 5e-05\t accuracy: 0.8515668153762818\n",
      "----------------------- 7/12 -----------------------\n",
      "momentum: 0.88\t weight_decay: 0.0001\t accuracy: 0.8563365817070008\n",
      "----------------------- 8/12 -----------------------\n",
      "momentum: 0.88\t weight_decay: 0.0005\t accuracy: 0.8532553553581238\n",
      "----------------------- 9/12 -----------------------\n",
      "momentum: 0.9\t weight_decay: 1e-05\t accuracy: 0.8476413249969482\n",
      "----------------------- 10/12 -----------------------\n",
      "momentum: 0.9\t weight_decay: 5e-05\t accuracy: 0.8580235242843628\n",
      "----------------------- 11/12 -----------------------\n",
      "momentum: 0.9\t weight_decay: 0.0001\t accuracy: 0.8524091124534607\n",
      "----------------------- 12/12 -----------------------\n",
      "momentum: 0.9\t weight_decay: 0.0005\t accuracy: 0.8532518029212952\n"
     ]
    }
   ],
   "source": [
    "momentum_values = [0.85, 0.88, 0.9]\n",
    "decay_values = [1e-5, 5e-5, 1e-4, 5e-4]\n",
    "\n",
    "num_iter = len(momentum_values) * len(decay_values)\n",
    "\n",
    "i = 1\n",
    "\n",
    "for momentum in momentum_values:\n",
    "    for decay in decay_values:\n",
    "        acc_list = []\n",
    "        print(f\"----------------------- {i}/{num_iter} -----------------------\")\n",
    "        for train_index, val_index in kf.split(X):\n",
    "            # Split the data into training and testing sets\n",
    "            X_train, X_val = X[train_index], X[val_index]\n",
    "            y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "            # Init the model in every iteration\n",
    "            model, early_stopping, lrs = model_init(\n",
    "                MyModel(l2=best_l2, dropout=best_dropout), best_lr, momentum, decay\n",
    "            )\n",
    "\n",
    "            history = model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                batch_size=best_batch,\n",
    "                epochs=EPOCHS,\n",
    "                callbacks=[early_stopping, lrs],\n",
    "                verbose=0,\n",
    "            )\n",
    "\n",
    "            acc_list.append(history.history[\"binary_accuracy\"][-1])\n",
    "\n",
    "        print(\n",
    "            f\"momentum: {momentum}\\t weight_decay: {decay}\\t accuracy: {mean(acc_list)}\"\n",
    "        )\n",
    "\n",
    "        # Create a DataFrame for the new row\n",
    "        new_row = pd.DataFrame(\n",
    "            [\n",
    "                [\n",
    "                    best_lr,\n",
    "                    best_batch,\n",
    "                    best_l2,\n",
    "                    best_dropout,\n",
    "                    momentum,\n",
    "                    decay,\n",
    "                    mean(acc_list),\n",
    "                ]\n",
    "            ],\n",
    "            columns=[\n",
    "                \"learning_rate\",\n",
    "                \"batch_size\",\n",
    "                \"l2_regularization\",\n",
    "                \"dropout_rate\",\n",
    "                \"momentum\",\n",
    "                \"weight_decay\",\n",
    "                \"accuracy\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Append the row using pd.concat()\n",
    "        accuracies_ft2_df = pd.concat([accuracies_ft2_df, new_row], ignore_index=True)\n",
    "        \n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "823fed6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>l2_regularization</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>momentum</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.859703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.858024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.856337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>0.853255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>0.853252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.852409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.851567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.850444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>0.850162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.848480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.847641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.845397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    learning_rate batch_size  l2_regularization  dropout_rate  momentum  \\\n",
       "4            0.01         32              0.005           0.1      0.88   \n",
       "9            0.01         32              0.005           0.1      0.90   \n",
       "6            0.01         32              0.005           0.1      0.88   \n",
       "7            0.01         32              0.005           0.1      0.88   \n",
       "11           0.01         32              0.005           0.1      0.90   \n",
       "10           0.01         32              0.005           0.1      0.90   \n",
       "5            0.01         32              0.005           0.1      0.88   \n",
       "0            0.01         32              0.005           0.1      0.85   \n",
       "3            0.01         32              0.005           0.1      0.85   \n",
       "2            0.01         32              0.005           0.1      0.85   \n",
       "8            0.01         32              0.005           0.1      0.90   \n",
       "1            0.01         32              0.005           0.1      0.85   \n",
       "\n",
       "    weight_decay  accuracy  \n",
       "4        0.00001  0.859703  \n",
       "9        0.00005  0.858024  \n",
       "6        0.00010  0.856337  \n",
       "7        0.00050  0.853255  \n",
       "11       0.00050  0.853252  \n",
       "10       0.00010  0.852409  \n",
       "5        0.00005  0.851567  \n",
       "0        0.00001  0.850444  \n",
       "3        0.00050  0.850162  \n",
       "2        0.00010  0.848480  \n",
       "8        0.00001  0.847641  \n",
       "1        0.00005  0.845397  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies_ft2_df.sort_values(by=\"accuracy\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cfad0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_ft2_df.to_csv(\"output/accuracies_fine_tuning_2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6598f8b4",
   "metadata": {},
   "source": [
    "Extract the parameters giving the highest value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9066974d",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_df_max_row = accuracies_ft2_df.loc[\n",
    "    accuracies_ft2_df[\"accuracy\"] == max(accuracies_ft2_df[\"accuracy\"])\n",
    "]\n",
    "best_momentum = accuracies_df_max_row[\"momentum\"].iloc[0]\n",
    "best_decay = accuracies_df_max_row[\"weight_decay\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1f5aa9",
   "metadata": {
    "papermill": {
     "duration": 0.10352,
     "end_time": "2024-09-18T17:34:55.747936",
     "exception": false,
     "start_time": "2024-09-18T17:34:55.644416",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submit prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05d5aaca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:34:55.957156Z",
     "iopub.status.busy": "2024-09-18T17:34:55.956757Z",
     "iopub.status.idle": "2024-09-18T17:34:58.806856Z",
     "shell.execute_reply": "2024-09-18T17:34:58.805781Z"
    },
    "papermill": {
     "duration": 2.957386,
     "end_time": "2024-09-18T17:34:58.809153",
     "exception": false,
     "start_time": "2024-09-18T17:34:55.851767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - binary_accuracy: 0.7202 - loss: 2.0372 - learning_rate: 0.0100\n",
      "Epoch 2/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.7923 - loss: 1.8043 - learning_rate: 0.0100\n",
      "Epoch 3/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.7755 - loss: 1.7380 - learning_rate: 0.0100\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kkleist\\OneDrive - DXC Production\\Desktop\\github\\Titanic-Competition\\titanic-comp-venv\\lib\\site-packages\\keras\\src\\callbacks\\early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: binary_accuracy,loss\n",
      "  current = self.get_monitor_value(logs)\n",
      "c:\\Users\\kkleist\\OneDrive - DXC Production\\Desktop\\github\\Titanic-Competition\\titanic-comp-venv\\lib\\site-packages\\keras\\src\\callbacks\\callback_list.py:96: UserWarning: Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: binary_accuracy,loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8492 - loss: 1.6227 - learning_rate: 0.0100\n",
      "Epoch 5/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8229 - loss: 1.6124 - learning_rate: 0.0100\n",
      "Epoch 6/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8351 - loss: 1.5278 - learning_rate: 0.0100\n",
      "Epoch 7/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.7984 - loss: 1.4886 - learning_rate: 0.0100\n",
      "Epoch 8/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8308 - loss: 1.4298 - learning_rate: 0.0100\n",
      "Epoch 9/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8209 - loss: 1.4020 - learning_rate: 0.0100\n",
      "Epoch 10/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8032 - loss: 1.3567 - learning_rate: 0.0100\n",
      "Epoch 11/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8214 - loss: 1.2798 - learning_rate: 0.0100\n",
      "Epoch 12/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8250 - loss: 1.2428 - learning_rate: 0.0100\n",
      "Epoch 13/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8172 - loss: 1.2176 - learning_rate: 0.0100\n",
      "Epoch 14/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8299 - loss: 1.1897 - learning_rate: 0.0100\n",
      "Epoch 15/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8196 - loss: 1.1712 - learning_rate: 0.0100\n",
      "Epoch 16/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8124 - loss: 1.1344 - learning_rate: 0.0100\n",
      "Epoch 17/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8098 - loss: 1.0962 - learning_rate: 0.0100\n",
      "Epoch 18/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8244 - loss: 1.0557 - learning_rate: 0.0100\n",
      "Epoch 19/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8075 - loss: 1.0151 - learning_rate: 0.0100\n",
      "Epoch 20/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8408 - loss: 0.9746 - learning_rate: 0.0100\n",
      "Epoch 21/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8148 - loss: 0.9863 - learning_rate: 0.0100\n",
      "Epoch 22/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8140 - loss: 0.9673 - learning_rate: 0.0100\n",
      "Epoch 23/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8083 - loss: 0.9395 - learning_rate: 0.0100\n",
      "Epoch 24/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8324 - loss: 0.8933 - learning_rate: 0.0100\n",
      "Epoch 25/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8250 - loss: 0.8866 - learning_rate: 0.0100\n",
      "Epoch 26/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8145 - loss: 0.8799 - learning_rate: 0.0100\n",
      "Epoch 27/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8083 - loss: 0.8614 - learning_rate: 0.0100\n",
      "Epoch 28/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8094 - loss: 0.8602 - learning_rate: 0.0100\n",
      "Epoch 29/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8347 - loss: 0.8022 - learning_rate: 0.0100\n",
      "Epoch 30/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8187 - loss: 0.7726 - learning_rate: 0.0100\n",
      "Epoch 31/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8344 - loss: 0.7646 - learning_rate: 0.0100\n",
      "Epoch 32/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.7966 - loss: 0.8087 - learning_rate: 0.0100\n",
      "Epoch 33/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8313 - loss: 0.7187 - learning_rate: 0.0100\n",
      "Epoch 34/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8251 - loss: 0.7174 - learning_rate: 0.0100\n",
      "Epoch 35/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8204 - loss: 0.6962 - learning_rate: 0.0100\n",
      "Epoch 36/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8458 - loss: 0.6656 - learning_rate: 0.0100\n",
      "Epoch 37/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 988us/step - binary_accuracy: 0.8278 - loss: 0.6772 - learning_rate: 0.0100\n",
      "Epoch 38/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8028 - loss: 0.7134 - learning_rate: 0.0100\n",
      "Epoch 39/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8502 - loss: 0.6415 - learning_rate: 0.0100\n",
      "Epoch 40/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8254 - loss: 0.6575 - learning_rate: 0.0100\n",
      "Epoch 41/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.7962 - loss: 0.7114 - learning_rate: 0.0100\n",
      "Epoch 42/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8653 - loss: 0.5896 - learning_rate: 0.0100\n",
      "Epoch 43/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8297 - loss: 0.6282 - learning_rate: 0.0100\n",
      "Epoch 44/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8410 - loss: 0.6022 - learning_rate: 0.0100\n",
      "Epoch 45/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8042 - loss: 0.6621 - learning_rate: 0.0100\n",
      "Epoch 46/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8346 - loss: 0.6009 - learning_rate: 0.0100\n",
      "Epoch 47/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8150 - loss: 0.6106 - learning_rate: 0.0100\n",
      "Epoch 48/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8325 - loss: 0.5627 - learning_rate: 0.0100\n",
      "Epoch 49/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8346 - loss: 0.5598 - learning_rate: 0.0100\n",
      "Epoch 50/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8461 - loss: 0.5512 - learning_rate: 0.0100\n",
      "Epoch 51/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8458 - loss: 0.5608 - learning_rate: 0.0100\n",
      "Epoch 52/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8226 - loss: 0.5551 - learning_rate: 0.0100\n",
      "Epoch 53/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8318 - loss: 0.5450 - learning_rate: 0.0100\n",
      "Epoch 54/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8578 - loss: 0.5108 - learning_rate: 0.0100\n",
      "Epoch 55/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8285 - loss: 0.5495 - learning_rate: 0.0100\n",
      "Epoch 56/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8332 - loss: 0.5418 - learning_rate: 0.0100\n",
      "Epoch 57/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8429 - loss: 0.5063 - learning_rate: 0.0100\n",
      "Epoch 58/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8311 - loss: 0.5196 - learning_rate: 0.0100\n",
      "Epoch 59/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8423 - loss: 0.5151 - learning_rate: 0.0100\n",
      "Epoch 60/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8228 - loss: 0.5310 - learning_rate: 0.0100\n",
      "Epoch 61/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8370 - loss: 0.5104 - learning_rate: 0.0100\n",
      "Epoch 62/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8040 - loss: 0.5578 - learning_rate: 0.0100\n",
      "Epoch 63/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8472 - loss: 0.4741 - learning_rate: 0.0100\n",
      "Epoch 64/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8422 - loss: 0.5300 - learning_rate: 0.0100\n",
      "Epoch 65/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8321 - loss: 0.5015 - learning_rate: 0.0100\n",
      "Epoch 66/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8223 - loss: 0.5069 - learning_rate: 0.0100\n",
      "Epoch 67/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8493 - loss: 0.4767 - learning_rate: 0.0100\n",
      "Epoch 68/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8180 - loss: 0.5005 - learning_rate: 0.0100\n",
      "Epoch 69/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8381 - loss: 0.5043 - learning_rate: 0.0100\n",
      "Epoch 70/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8262 - loss: 0.5146 - learning_rate: 0.0100\n",
      "Epoch 71/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8171 - loss: 0.5073 - learning_rate: 0.0100\n",
      "Epoch 72/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8508 - loss: 0.4683 - learning_rate: 0.0100\n",
      "Epoch 73/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8535 - loss: 0.4672 - learning_rate: 0.0100\n",
      "Epoch 74/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8490 - loss: 0.4671 - learning_rate: 0.0100\n",
      "Epoch 75/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8260 - loss: 0.5007 - learning_rate: 0.0100\n",
      "Epoch 76/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8298 - loss: 0.4791 - learning_rate: 0.0100\n",
      "Epoch 77/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8036 - loss: 0.5024 - learning_rate: 0.0100\n",
      "Epoch 78/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8443 - loss: 0.4587 - learning_rate: 0.0100\n",
      "Epoch 79/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8540 - loss: 0.4484 - learning_rate: 0.0100\n",
      "Epoch 80/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8224 - loss: 0.4946 - learning_rate: 0.0100\n",
      "Epoch 81/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8408 - loss: 0.4369 - learning_rate: 0.0100\n",
      "Epoch 82/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8228 - loss: 0.4497 - learning_rate: 0.0100\n",
      "Epoch 83/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8218 - loss: 0.4428 - learning_rate: 0.0100\n",
      "Epoch 84/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8461 - loss: 0.4351 - learning_rate: 0.0100\n",
      "Epoch 85/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8378 - loss: 0.4495 - learning_rate: 0.0100\n",
      "Epoch 86/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8371 - loss: 0.4388 - learning_rate: 0.0100\n",
      "Epoch 87/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8413 - loss: 0.4374 - learning_rate: 0.0100\n",
      "Epoch 88/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8355 - loss: 0.4547 - learning_rate: 0.0100\n",
      "Epoch 89/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8382 - loss: 0.4512 - learning_rate: 0.0100\n",
      "Epoch 90/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8110 - loss: 0.4904 - learning_rate: 0.0100\n",
      "Epoch 91/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8416 - loss: 0.4328 - learning_rate: 0.0100\n",
      "Epoch 92/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8230 - loss: 0.4742 - learning_rate: 0.0100\n",
      "Epoch 93/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8379 - loss: 0.4506 - learning_rate: 0.0100\n",
      "Epoch 94/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8293 - loss: 0.4655 - learning_rate: 0.0100\n",
      "Epoch 95/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8354 - loss: 0.4489 - learning_rate: 0.0100\n",
      "Epoch 96/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8366 - loss: 0.4532 - learning_rate: 0.0100\n",
      "Epoch 97/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8208 - loss: 0.4548 - learning_rate: 0.0100\n",
      "Epoch 98/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8358 - loss: 0.4859 - learning_rate: 0.0100\n",
      "Epoch 99/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8253 - loss: 0.4761 - learning_rate: 0.0100\n",
      "Epoch 100/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8291 - loss: 0.4396 - learning_rate: 0.0100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Your submission was successfully saved!\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "model, early_stopping, lrs = model_init(\n",
    "    MyModel(l2=best_l2, dropout=best_dropout), best_lr, best_momentum, best_decay\n",
    ")\n",
    "model.fit(\n",
    "    X,\n",
    "    y,\n",
    "    batch_size=best_batch,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stopping, lrs],\n",
    ")\n",
    "\n",
    "# Get predictions\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(np.intc)\n",
    "predictions = np.concatenate(y_pred)\n",
    "\n",
    "# Saving the predictions\n",
    "output = pd.DataFrame({\"PassengerId\": test_data.index, \"Survived\": predictions})\n",
    "if not os.path.isdir(\"output/\"):\n",
    "    os.mkdir(\"output/\")\n",
    "output.to_csv(\"output/submission.csv\", index=False)\n",
    "\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc46bbad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 26502,
     "sourceId": 3136,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "titanic-comp-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 77.245806,
   "end_time": "2024-09-18T17:35:01.531921",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-18T17:33:44.286115",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
